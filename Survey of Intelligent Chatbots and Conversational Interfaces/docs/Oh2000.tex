\subsection{Stochastic Language Generation for Spoken Dialogue Systems \cite{Oh2000}}

This paper proposes a new corpus-based approach to natural language generation, specifically designed for \emph{\emph{Spoken Dialogue Systems (SDSs)}}. This work is motivated by the observation that the two current approaches to language generation, \emph{template-based} and \emph{rule-based} NLG have limitations when applied to SDSs.

The basic idea of this paper is to develop a corpus-based generation system, in which it models language spoken by domain experts performing the task of interest, and uses that model to stochastically generate system utterance. It used two corpora in the travel reservations domain. One corpus consists of 39 dialogues, and another corpus consists of 68 dialogues.

\begin{figure}[h]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=.6\linewidth]{Oh00-NLG_overall_arch.png}\\
  \caption{Overall architecture.}\label{fig:Oh00-NLG_overall_arch}
\end{figure}

The overall architecture of the NLG system is shown in Figure \ref{fig:Oh00-NLG_overall_arch}. The \emph{content planning} components decides which attributes should be included in an utterance, and the \emph{surface realization} component decides how to translate the attributes to natural language. Next we will introduce these two components in more details.

The paper presents two approaches for content planning. The first approach, called \emph{old versus new}, selects only new information to be included in the system utterance. The second approach is to use a statistical model. The model first predicts the number of attributes in the system utterance, by learning the probability distribution $P(n_k) = P(n_k | c_k)$, where $n_k$ is the number of attributes and $c_k$ is the utterance class for system utterance $k$. The second step of the model is to select the attributes by
$$A^* = \mathop{\arg \max}_{a_1, ..., a_n} \sum_{k=1}^m P(b_k) \prod_{i=1}^n P(a_i | b_k),$$
where $\{b_1, ..., b_m\}$ is the set of $m$ attributes in the preceding user utterance.

The stochastic surface realization component uses different levels of generation granularity to various utterance classes. For example the hello message can be simply generated by a fixed expression, while the more complex output is generated by a statistical language model. There are four aspects of the surface realizer which will be discussed in what follows: building language models, generating candidate utterances, scoring the utterances, and filling in the slots.

When building the language model, the system first replaces tokens by their word classes (e.g. ``U.S. Airways'' by ``airline''). Then it uses a 5-gram model to introduce some variability in the output utterances while preventing nonsense responses.

The input to NLG from the dialogue manager is a frame of attribute-values pairs. The generation engine uses the appropriate language model for the utterance class and generates word sequences randomly according to the language model distributions.

For each randomly generated utterance, it computes a penalty score. The score is based on the heuristics that are empirically selected. The generation engine generates a candidate utterance, scores it, keeping only the best-scored utterance. It stops and returns the best utterance when it finds an utterance with a zero penalty score, or runs out of time.

The last step is filling slots with the appropriate values. For example, the utterance ``What time would you like to leave \{depart\_city\}?'' becomes ``What time would you like to leave New York?''.

In the experimental study, the paper conducts a comparative evaluation by running two identical systems varying only the generation component. Twelves subjects were involved in the experiment. It is shown that the subjects prefer the new versus old content planning approach, and the stochastic generation model. However, both the results are not statistically significant. The authors were still in the process of designing a larger evaluation.
