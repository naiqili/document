\subsection{Neural responding machine for short-text conversation \cite{Shang2015Neural}.}

The task is to build end-to-end trainable chatbots that can have one round conversations. The paper applies encoder-decoder framework, which uses GRU \cite{Chung2014Empirical, Cho2014Learning} encoders to encode the input post to a hidden state, and uses a GRU decoder to decode the output reply.

The encoders consist of a global encoder \cite{Chung2014Empirical, Cho2014Learning} and a local encoder \cite{Bahdanau2014Neural, Graves2013Generating}. The global encoder maps the input post to the last hidden state:
\begin{equation}
h_{t} \ = \ f( h_{t-1}, x_{t} ) \\
\end{equation}
where ($x_{1},...,x_{T}$) is a input sequence of words. The decoder uses $h_{T}$ as the context vector $c_{t}$

The local encoder maps the input post to weighted sum of hidden states when encoding the input post:
\begin{equation}
\begin{aligned}
h_{t} \ =& \ f( h_{t-1}, x_{t} ) \\
\alpha_{tj} \ =& \ q(s_{t-1}, h_{j}) \\
c_{t} \ =& \ \sum_{j=1}^{T} \alpha_{tj}h_{j} \\
\end{aligned}
\end{equation}

The decoder works as follows:
\begin{equation}
\begin{aligned}
s_{t} \ =& \ f( s_{t-1}, y_{t-1}, c_{t} ) \\
y_{t} \ =& \ g( s_{t} ) \\
\end{aligned}
\end{equation}