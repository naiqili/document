Automatically generated by Mendeley Desktop 1.15.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Leggetter1995,
abstract = {A method of speaker adaptation for continuous density hidden Markov models (HMMs) is presented. An initial speaker-independent system is adapted to improve the modelling of a new speaker by updating the HMM parameters. Statistics are gathered from the available adaptation data and used to calculate a linear regression-based transformation for the mean vectors. The transformation matrices are calculated to maximize the likelihood of the adaptation data and can be implemented using the forward–backward algorithm. By tying the transformations among a number of distributions, adaptation can be performed for distributions which are not represented in the training data. An important feature of the method is that arbitrary adaptation data can be used—no special enrolment sentences are needed. Experiments have been performed on the ARPA RM1 database using an HMM system with cross-word triphones and mixture Gaussian output distributions. Results show that adaptation can be performed using as little as 11 s of adaptation data, and that as more data is used the adaptation performance improves. For example, using 40 adaptation utterances, a 37{\%} reduction in error from the speaker-independent system was achieved with supervised adaptation and a 32{\%} reduction in unsupervised mode.},
author = {Leggetter, C.J. and Woodland, P.C.},
doi = {10.1006/csla.1995.0010},
file = {:D$\backslash$:/Research/papers/10.1.1.89.2050.pdf:pdf},
isbn = {0885-2308},
issn = {08852308},
journal = {Computer Speech {\&} Language},
number = {2},
pages = {171--185},
title = {{Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models}},
url = {http://www.sciencedirect.com/science/article/pii/S0885230885700101},
volume = {9},
year = {1995}
}
@article{Williams2016End,
abstract = {This paper presents a model for end-to-end learning of task-oriented dialog systems. The main component of the model is a recurrent neural network (an LSTM), which maps from raw dialog history directly to a distribution over system actions. The LSTM automatically infers a representation of dialog history, which relieves the system developer of much of the manual feature engineering of dialog state. In addition, the developer can provide software that expresses business rules and provides access to programmatic APIs, enabling the LSTM to take actions in the real world on behalf of the user. The LSTM can be optimized using supervised learning (SL), where a domain expert provides example dialogs which the LSTM should imitate; or using reinforcement learning (RL), where the system improves by interacting directly with end users. Experiments show that SL and RL are complementary: SL alone can derive a reasonable initial policy from a small number of training dialogs; and starting RL optimization with a policy trained with SL substantially accelerates the learning rate of RL.},
archivePrefix = {arXiv},
arxivId = {1606.01269},
author = {Williams, Jason D. and Zweig, Geoffrey},
doi = {arXiv:1504.01391},
eprint = {1606.01269},
file = {:D$\backslash$:/Research/papers/williams2016lstm.pdf:pdf},
journal = {arXiv},
title = {{End-to-end LSTM-based dialog control optimized with supervised and reinforcement learning}},
url = {http://arxiv.org/abs/1606.01269},
year = {2016}
}
@article{Sainath2013,
abstract = {Convolutional Neural Networks (CNNs) are an alternative type of neural network that can be used to reduce spectral variations and model spectral correlations which exist in signals. Since speech signals exhibit both of these properties, CNNs are a more effective model for speech compared to Deep Neural Networks (DNNs). In this paper, we explore applying CNNs to large vocabulary speech tasks. First, we determine the appropriate architecture to make CNNs effective compared to DNNs for LVCSR tasks. Specifically, we focus on how many convolutional layers are needed, what is the optimal number of hidden units, what is the best pooling strategy, and the best input feature type for CNNs. We then explore the be- havior of neural network features extracted from CNNs on a vari- ety of LVCSR tasks, comparing CNNs to DNNs and GMMs. We find that CNNs offer between a 13-30{\%} relative improvement over GMMs, and a 4-12{\%} relative improvement over DNNs, on a 400-hr Broadcast News and 300-hr Switchboard task.},
archivePrefix = {arXiv},
arxivId = {1309.1501},
author = {Sainath, Tara N. and Mohamed, Abdel Rahman and Kingsbury, Brian and Ramabhadran, Bhuvana},
doi = {10.1109/ICASSP.2013.6639347},
eprint = {1309.1501},
file = {:D$\backslash$:/Research/papers/icassp13{\_}cnn.pdf:pdf},
isbn = {9781479903566},
issn = {15206149},
journal = {Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
keywords = {Neural Networks,Speech Recognition},
pages = {8614--8618},
title = {{Deep convolutional neural networks for LVCSR}},
year = {2013}
}
@article{Wu2016Investigating,
abstract = {Recently, recurrent neural networks (RNNs) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis (SPSS). The long short-term memory (LSTM) architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve significantly better performance on SPSS than deep feed-forward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component (e.g., input gate, output gate, forget gate) is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simplified architecture. The simplified architecture has significantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality.},
archivePrefix = {arXiv},
arxivId = {1601.02539},
author = {Wu, Zhizheng and King, Simon},
doi = {10.1109/ICASSP.2016.7472657},
eprint = {1601.02539},
file = {:D$\backslash$:/Research/papers/1601.02539v1.pdf:pdf},
isbn = {9781479999880},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Speech synthesis,acoustic modelling,gated recurrent network,long short-term memory,recurrent network network},
pages = {5140--5144},
title = {{Investigating gated recurrent networks for speech synthesis}},
volume = {2016-May},
year = {2016}
}
@article{Martens2011,
abstract = {Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely diffi- cult to train them properly. Fortunately, re- cent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence prob- lems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free op- timizer (HF) by applying them to character-level language modeling tasks. The standard RNN ar- chitecture, while effective, is not ideally suited for such tasks, so we introduce a new RNN variant that uses multiplicative (or gated) con- nections which allow the current input charac- ter to determine the transition matrix from one hidden state vector to the next. After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units, we were able to surpass the performance of the best previous single method for character- level language modeling a hierarchical non- parametric sequence model. To our knowledge this represents the largest recurrent neural net- work application to date.},
author = {Martens, James},
doi = {2},
file = {:D$\backslash$:/Research/papers/LANG-RNN.pdf:pdf},
isbn = {9781450306195},
issn = {1},
journal = {Neural Networks},
number = {1},
pages = {1017--1024},
title = {{Generating Text with Recurrent Neural Networks}},
url = {http://www.icml-2011.org/papers/524{\_}icmlpaper.pdf},
volume = {131},
year = {2011}
}
@article{Ritter2011,
abstract = {We present a data-driven approach to generating responses to Twitter status posts, based on phrase-based Statistical Machine Translation. We find that mapping conversational stimuli onto responses is more difficult than translating between languages, due to the wider range of possible responses, the larger fraction of unaligned words/phrases, and the presence of large phrase pairs whose alignment cannot be further decomposed. After addressing these challenges, we compare approaches based on SMT and Information Retrieval in a human evaluation. We show that SMT outperforms IR on this task, and its output is preferred over actual human responses in 15{\%} of cases. As far as we are aware, this is the first work to investigate the use of phrase-based SMT to directly translate a linguistic stimulus into an appropriate response.},
author = {Ritter, Alan and Cherry, Colin and Dolan, William B.},
file = {:D$\backslash$:/Research/papers/mt{\_}chat.pdf:pdf},
isbn = {978-1-937284-11-4},
journal = {Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP'11)},
pages = {583--593},
title = {{Data-driven response generation in social media}},
url = {http://dl.acm.org/citation.cfm?id=2145500},
year = {2011}
}
@article{Rabiner1989A,
abstract = {This tutorial provides an overview of the basic theory of hidden$\backslash$nMarkov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and$\backslash$ngives practical details on methods of implementation of the theory along$\backslash$nwith a description of selected applications of the theory to distinct$\backslash$nproblems in speech recognition. Results from a number of original$\backslash$nsources are combined to provide a single source of acquiring the$\backslash$nbackground required to pursue further this area of research. The author$\backslash$nfirst reviews the theory of discrete Markov chains and shows how the$\backslash$nconcept of hidden states, where the observation is a probabilistic$\backslash$nfunction of the state, can be used effectively. The theory is$\backslash$nillustrated with two simple examples, namely coin-tossing, and the$\backslash$nclassic balls-in-urns system. Three fundamental problems of HMMs are$\backslash$nnoted and several practical techniques for solving these problems are$\backslash$ngiven. The various types of HMMs that have been studied, including$\backslash$nergodic as well as left-right models, are described},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rabiner, L.R.},
doi = {10.1109/5.18626},
eprint = {arXiv:1011.1669v3},
file = {:D$\backslash$:/Research/papers/rabiner{\_}hmm.pdf:pdf},
isbn = {1558601244},
issn = {00189219},
journal = {Proceedings of the IEEE},
number = {2},
pages = {257--286},
pmid = {21920608},
title = {{A tutorial on hidden Markov models and selected applications in speech recognition}},
volume = {77},
year = {1989}
}
@article{Levin2000A,
abstract = {We propose a quantitative model for dialog systems that can be$\backslash$nused for learning the dialog strategy. We claim that the problem of$\backslash$ndialog design can be formalized as an optimization problem with an$\backslash$nobjective function reflecting different dialog dimensions relevant for a$\backslash$ngiven application. We also show that any dialog system can be formally$\backslash$ndescribed as a sequential decision process in terms of its state space,$\backslash$naction set, and strategy. With additional assumptions about the state$\backslash$ntransition probabilities and cost assignment, a dialog system can be$\backslash$nmapped to a stochastic model known as Markov decision process (MDP). A$\backslash$nvariety of data driven algorithms for finding the optimal strategy$\backslash$n(i.e., the one that optimizes the criterion) is available within the MDP$\backslash$nframework, based on reinforcement learning. For an effective use of the$\backslash$navailable training data we propose a combination of supervised and$\backslash$nreinforcement learning: the supervised learning is used to estimate a$\backslash$nmodel of the user, i.e., the MDP parameters that quantify the user's$\backslash$nbehavior. Then a reinforcement learning algorithm is used to estimate$\backslash$nthe optimal strategy while the system interacts with the simulated user.$\backslash$nThis approach is tested for learning the strategy in an air travel$\backslash$ninformation system (ATIS) task. The experimental results we present in$\backslash$nthis paper show that it is indeed possible to find a simple criterion, a$\backslash$nstate space representation, and a simulated user parameterization in$\backslash$norder to automatically learn a relatively complex dialog behavior,$\backslash$nsimilar to one that was heuristically designed by several research$\backslash$ngroups},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Levin, Esther and Pieraccini, Roberto and Eckert, Wieland},
doi = {10.1109/89.817450},
eprint = {9809069v1},
file = {:D$\backslash$:/Research/papers/IEEE{\_}TSAP{\_}00.pdf:pdf},
isbn = {1063-6676 VO - 8},
issn = {10636676},
journal = {IEEE Transactions on Speech and Audio Processing},
keywords = {Dialog systems,Markov decision process,Reinforcement learning,Sequential decision process,Speech,Spoken language systems},
number = {1},
pages = {11--23},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
title = {{A stochastic model of human-machine interaction for learning dialog strategies}},
volume = {8},
year = {2000}
}
@article{Henderson2013,
abstract = {While belief tracking is known to be im-portant in allowing statistical dialog sys-tems to manage dialogs in a highly robust manner, until recently little attention has been given to analysing the behaviour of belief tracking techniques. The Dialogue State Tracking Challenge has allowed for such an analysis, comparing multiple be-lief tracking approaches on a shared task. Recent success in using deep learning for speech research motivates the Deep Neu-ral Network approach presented here. The model parameters can be learnt by directly maximising the likelihood of the training data. The paper explores some aspects of the training, and the resulting tracker is found to perform competitively, particu-larly on a corpus of dialogs from a system not found in the training.},
author = {Henderson, Matthew and Thomson, Blaise and Young, Steve},
file = {:D$\backslash$:/Research/papers/W13-4073.pdf:pdf},
isbn = {9781937284954},
journal = {Proceedings of the SIGDIAL 2013 Conference},
pages = {467--471},
title = {{Deep Neural Network Approach for the Dialog State Tracking Challenge}},
url = {http://www.aclweb.org/anthology/W/W13/W13-4073},
year = {2013}
}
@article{Thomson2010,
abstract = {This paper describes a statistically motivated framework for performing real-time dialogue state updates and policy learning in a spoken dialogue system. The framework is based on the partially observable Markov decision process (POMDP), which provides a well-founded, statistical model of spoken dialogue management. However, exact belief state updates in a POMDP model are computationally intractable so approximate methods must be used. This paper presents a tractable method based on the loopy belief propagation algorithm. Various simplifications are made, which improve the efficiency significantly compared to the original algorithm as well as compared to other POMDP-based dialogue state updating approaches. A second contribution of this paper is a method for learning in spoken dialogue systems which uses a component-based policy with the episodic Natural Actor Critic algorithm. The framework proposed in this paper was tested on both simulations and in a user trial. Both indicated that using Bayesian updates of the dialogue state significantly outperforms traditional definitions of the dialogue state. Policy learning worked effectively and the learned policy outperformed all others on simulations. In user trials the learned policy was also competitive, although its optimality was less conclusive. Overall, the Bayesian update of dialogue state framework was shown to be a feasible and effective approach to building real-world POMDP-based dialogue systems. ?? 2009 Elsevier Ltd. All rights reserved.},
author = {Thomson, Blaise and Young, Steve},
doi = {10.1016/j.csl.2009.07.003},
file = {:D$\backslash$:/Research/papers/csl-buds.pdf:pdf},
isbn = {0885-2308},
issn = {08852308},
journal = {Computer Speech and Language},
keywords = {Dialogue systems,POMDP,Reinforcement learning,Robustness},
number = {4},
pages = {562--588},
title = {{Bayesian update of dialogue state: A POMDP framework for spoken dialogue systems}},
volume = {24},
year = {2010}
}
@article{Ljolje1999,
author = {Ljolje, Andrej and Pereira, Fernando and Riley, Michael and Avenue, Park and Park, Florham},
file = {:D$\backslash$:/Research/papers/R030.PDF:PDF},
journal = {Eurospeech},
pages = {1--4},
title = {{Efficient general lattice generation and rescoring.}},
url = {http://20.210-193-52.unknown.qala.com.sg/archive/archive{\_}papers/eurospeech{\_}1999/e99{\_}1251.pdf},
year = {1999}
}
@article{Zilka2015,
archivePrefix = {arXiv},
arxivId = {1507.03471},
author = {Zilka, Luk{\'{a}}s and Jurcicek, Filip},
eprint = {1507.03471},
file = {:D$\backslash$:/Research/papers/1507.03471v1.pdf:pdf},
journal = {CoRR},
number = {L},
title = {{Incremental LSTM-based Dialog State Tracker}},
url = {http://arxiv.org/abs/1507.03471},
volume = {abs/1507.0},
year = {2015}
}
@article{Pascanu2012Understanding,
abstract = {There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and the exploding gradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:D$\backslash$:/Research/papers/1211.5063v1.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {1045-9227},
journal = {Proceedings of The 30th International Conference on Machine Learning},
number = {2},
pages = {1310--1318},
pmid = {18267787},
title = {{On the difficulty of training recurrent neural networks}},
url = {http://jmlr.org/proceedings/papers/v28/pascanu13.pdf},
year = {2012}
}
@article{Vesely2013,
abstract = {Sequence-discriminative training of deep neural networks (DNNs) is investigated on a 300 hour American English conversational telephone speech task. Different sequence-discriminative criteria . maximum mutual information (MMI), minimum phone error (MPE), state-level minimum Bayes risk (sMBR), and boosted MMI . are compared. Two different heuristics are investigated to improve the performance of the DNNs trained using sequence-based criteria . lattices are regenerated after the first iteration of training; and, for MMI and BMMI, the frames where the numerator and denominator hypotheses are disjoint are removed from the gradient computation. Starting from a competitive DNN baseline trained using cross-entropy, different sequence-discriminative criteria are shown to lower word error rates by 8.9{\%} relative, on average. Little difference is noticed between the different sequence-based criteria that are investigated. The experiments are done using the open-source Kaldi toolkit, which makes it possible for the wider community to reproduce these results.},
author = {Vesel{\'{y}}, Karel and Ghoshal, Arnab and Burget, Luk{\'{a}}{\v{s}} and Povey, Daniel},
doi = {10.1109/ICASSP.2013.6639310},
file = {:D$\backslash$:/Research/papers/vesely{\_}interspeech2013{\_}IS131333.pdf:pdf},
isbn = {9781479927562},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Deep learning,Neural networks,Reproducible research,Sequence-criterion training,Speech recognition},
number = {August},
pages = {2345--2349},
title = {{Sequence-discriminative training of deep neural networks}},
year = {2013}
}
@article{Henderson2013a,
author = {Henderson, Matthew and Thomson, Blaise and Williams, Jason},
file = {:D$\backslash$:/Research/papers/handbook.pdf:pdf},
isbn = {9781479971299},
number = {September},
pages = {1--22},
title = {{Dialog State Tracking Challenge 2 {\&} 3}},
year = {2013}
}
@article{Sordoni2015a,
abstract = {Users may strive to formulate an adequate textual query for their information need. Search engines assist the users by presenting query suggestions. To preserve the original search intent, suggestions should be context-aware and account for the previous queries issued by the user. Achieving context awareness is challenging due to data sparsity. We present a probabilistic suggestion model that is able to account for sequences of previous queries of arbitrary lengths. Our novel hierarchical recurrent encoder-decoder architecture allows the model to be sensitive to the order of queries in the context while avoiding data sparsity. Additionally, our model can suggest for rare, or long-tail, queries. The produced suggestions are synthetic and are sampled one word at a time, using computationally cheap decoding techniques. This is in contrast to current synthetic suggestion models relying upon machine learning pipelines and hand-engineered feature sets. Results show that it outperforms existing context-aware approaches in a next query prediction setting. In addition to query suggestion, our model is general enough to be used in a variety of other applications.},
archivePrefix = {arXiv},
arxivId = {1507.02221},
author = {Sordoni, Alessandro and Bengio, Yoshua and Vahabi, Hossein and Lioma, Christina and Simonsen, Jakob G. and Nie, Jian-Yun},
doi = {10.1145/XXX.XXXXXXX},
eprint = {1507.02221},
file = {:D$\backslash$:/Research/papers/1507.02221v1 (1).pdf:pdf},
isbn = {9781450337946},
journal = {Cikm-2015},
keywords = {query suggestion,recurrent neural networks},
pages = {553--562},
title = {{A Hierarchical Recurrent Encoder-Decoder For Generative Context-Aware Query Suggestion}},
url = {http://arxiv.org/abs/1507.02221},
year = {2015}
}
@article{Sordoni2015,
abstract = {We present a novel response generation sys-tem that can be trained end to end on large quantities of unstructured Twitter conversa-tions. A neural network architecture is used to address sparsity issues that arise when in-tegrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show con-sistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.},
archivePrefix = {arXiv},
arxivId = {1506.06714},
author = {Sordoni, Alessandro and Galley, Michel and Auli, Michael and Brockett, Chris and Ji, Yangfeng and Mitchell, Margaret and Nie, Jian-Yun and Gao, Jianfeng and Dolan, William B.},
eprint = {1506.06714},
file = {:D$\backslash$:/Research/papers/1506.06714v1.pdf:pdf},
isbn = {9781941643495},
journal = {Naacl-2015},
pages = {196--205},
title = {{A Neural Network Approach to Context-Sensitive Generation of Conversational Responses}},
year = {2015}
}
@article{Bengio2003A,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal and Janvin, Christian},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
file = {:D$\backslash$:/Research/papers/bengio03a.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {The Journal of Machine Learning Research},
keywords = {artificial neural networks,curse of dimensionality,distributed representation,statistical language modeling},
pages = {1137--1155},
pmid = {18244602},
title = {{A Neural Probabilistic Language Model}},
volume = {3},
year = {2003}
}
@article{Henderson2014Word,
abstract = {Recently discriminative methods for track-ing the state of a spoken dialog have been shown to outperform traditional generative models. This paper presents a new word-based tracking method which maps di-rectly from the speech recognition results to the dialog state without using an explicit semantic decoder. The method is based on a recurrent neural network structure which is capable of generalising to unseen dialog state hypotheses, and which requires very little feature engineering. The method is evaluated on the second Dialog State Tracking Challenge (DSTC2) corpus and the results demonstrate consistently high performance across all of the metrics.},
author = {Henderson, Matthew and Thomson, Blaise and Young, Steve},
file = {:D$\backslash$:/Research/papers/Word{\_}based{\_}Dialog{\_}State{\_}Tracking{\_}with{\_}Recurrent{\_}Neural{\_}Networks.pdf:pdf},
journal = {Sigdial'14},
pages = {292--299},
title = {{Word-Based Dialog State Tracking with Recurrent Neural Networks}},
url = {http://www.aclweb.org/anthology/W14-4340},
year = {2014}
}
@article{Serban2016,
abstract = {We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings.},
archivePrefix = {arXiv},
arxivId = {1507.04808},
author = {Serban, Iulian V. and Sordoni, Alessandro and Bengio, Yoshua and Courville, Aaron and Pineau, Joelle},
eprint = {1507.04808},
file = {:D$\backslash$:/Research/papers/1507.04808v3.pdf:pdf},
journal = {Aaai},
pages = {8},
title = {{Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models}},
url = {http://arxiv.org/abs/1507.04808},
year = {2016}
}
@article{Povey2012,
abstract = {We describe a lattice generation method that is exact, i.e. it satisfies all the natural properties we would want from a lattice of alterna- tive transcriptions of an utterance. This method does not introduce substantial overhead above one-best decoding. Our method is most directly applicable when using WFST decoders where the WFST is expanded down to the HMM-state level. It outputs lattices that in- clude state-level alignments as well as word labels. The general idea is to create a state-level lattice during decoding, and to do a special form of determinization that retains only the best-scoring path for each word sequence.},
author = {Povey, Daniel and Hannemann, Mirko and Boulianne, Gilles and Burget, Luk{\'{a}}{\v{s}} and Ghoshal, Arnab and Janda, Milo{\v{s}} and Karafi{\'{a}}t, Martin and Kombrink, Stefan and Motl{\'{\i}}{\v{c}}ek, Petr and Qian, Yanmin and Riedhammer, Korbinian and Vesel{\'{y}}, Karel and Vu, Ngoc Thang},
doi = {10.1109/ICASSP.2012.6288848},
file = {:D$\backslash$:/Research/papers/2012{\_}icassp{\_}lattices.pdf:pdf},
isbn = {9781467300469},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Lattice Generation,Speech Recognition},
number = {102},
pages = {4213--4216},
title = {{Generating exact lattices in the WFST framework}},
volume = {213850},
year = {2012}
}
@article{Gasic2011,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
author = {Ga{\v{s}}i{\'{c}}, Milica and Jur{\v{c}}i{\v{c}}ek, Filip and Thomson, Blaise and Yu, Kai and Young, Steve},
doi = {10.1109/ASRU.2011.6163950},
file = {:D$\backslash$:/Research/papers/gponline.pdf:pdf},
isbn = {9781467303675},
issn = {1098-6596},
journal = {2011 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2011, Proceedings},
pages = {312--317},
title = {{On-line policy optimisation of spoken dialogue systems via live interaction with human subjects}},
year = {2011}
}
@article{Hinton2012Deep,
abstract = {Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.},
archivePrefix = {arXiv},
arxivId = {1207.0580},
author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and Kingsbury, Brian},
doi = {10.1109/MSP.2012.2205597},
eprint = {1207.0580},
file = {:D$\backslash$:/Research/papers/HintonDengYuEtAl-SPM2012.pdf:pdf},
isbn = {1053-5888},
issn = {1053-5888},
journal = {Ieee Signal Processing Magazine},
number = {November},
pages = {82--97},
pmid = {13057166},
title = {{Deep Neural Networks for Acoustic Modeling in Speech Recognition}},
year = {2012}
}
@misc{Sak2010,
abstract = {This paper presents a method for rescoring the speech recog- nition lattices on-the-fly to increase the word accuracy while preserving low latency of a real-time speech recognition sys- tem. In large vocabulary speech recognition systems, pruned and/or lower order n-gram language models are often used in the first-pass of the speech decoder due to the computational complexity. The output word lattices are rescored offline with a better language model to improve the accuracy. For real-time speech recognition systems, offline lattice rescoring increases the latency of the system and may not be appropriate. We pro- pose a method for on-the-fly lattice rescoring and generation, and evaluate it on a broadcast speech recognition task. This first-pass lattice rescoring method can generate rescored lattices with less than 20{\%} increased computation over standard lattice generation without increasing the latency of the system.},
author = {Sak, Haşim and Sara{\c{c}}lar, Murat and G{\"{u}}ng{\"{o}}r, T},
booktitle = {Interspeech},
file = {:D$\backslash$:/Research/papers/On-the-ﬂy Lattice Rescoring for Real-time Automatic Speech Recognition.pdf:pdf},
pages = {1--4},
title = {{On-the-fly lattice rescoring for real-time automatic speech recognition.}},
url = {http://20.210-193-52.unknown.qala.com.sg/archive/archive{\_}papers/interspeech{\_}2010/i10{\_}2450.pdf},
year = {2010}
}
@article{Graves2013Speech,
abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates $\backslash$emph{\{}deep recurrent neural networks{\}}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7{\%} on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
archivePrefix = {arXiv},
arxivId = {arXiv:1303.5778v1},
author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
doi = {10.1109/ICASSP.2013.6638947},
eprint = {arXiv:1303.5778v1},
file = {:D$\backslash$:/Research/papers/RNN13.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {1520-6149},
journal = {Icassp},
number = {3},
pages = {6645--6649},
title = {{Speech Recognition With Deep Recurrent Neural Networks}},
year = {2013}
}
@article{Hwang2001,
author = {Hwang, Tai-hwei and Yuo, Kuo-hwei and Wang, Hsiao-chuan},
file = {:D$\backslash$:/Research/papers/page877.pdf:pdf},
title = {{Linear Interpolation of Cepstral Variance for Noisy Speech Recognition}},
year = {2001}
}
@article{Serban2015,
abstract = {During the past decade, several areas of speech and language understanding have witnessed substantial breakthroughs from the use of data-driven models. In the area of dialogue systems, the trend is less obvious, and most practical systems are still built through significant engineering and expert knowledge. Nevertheless, several recent results suggest that data-driven approaches are feasible and quite promising. To facilitate research in this area, we have carried out a wide survey of publicly available datasets suitable for data-driven learning of dialogue systems. We discuss important characteristics of these datasets and how they can be used to learn diverse dialogue strategies. We also describe other potential uses of these datasets, such as methods for transfer learning between datasets and the use of external knowledge, and discuss appropriate choice of evaluation metrics for the learning objective.},
archivePrefix = {arXiv},
arxivId = {1512.05742},
author = {Serban, Iulian Vlad and Lowe, Ryan and Charlin, Laurent and Pineau, Joelle},
eprint = {1512.05742},
file = {:D$\backslash$:/Research/papers/1512.05742v2.pdf:pdf},
pages = {46},
title = {{A Survey of Available Corpora for Building Data-Driven Dialogue Systems}},
url = {http://arxiv.org/abs/1512.05742},
year = {2015}
}
@article{Lison2013,
abstract = {This thesis presents a new modelling framework for dialogue management based on the concept of probabilistic rules. Probabilistic rules are defined as if...then...else constructions associating logical conditions on input variables to probabilistic effects over output variables. These rules function as high-level templates for the generation of a directed graphical model. Their expressive power allows them to represent the probabilistic models employed in dialogue management in a compact and efficient manner. As a consequence, they can drastically reduce the amount of interaction data required for parameter estimation as well as enhance the system’s ability to generalise over unseen situations. Furthermore, probabilistic rules can also be exploited to encode domain-specific constraints and assumptions into statistical models of dialogue, thereby enabling system designers to incorporate their expert knowledge of the problem structure in a concise and human-readable form. Due to their integration of logical and probabilistic reasoning, we argue that probabilistic rules are particularly well suited to devise hybrid models of dialogue management that can account for both the complexity and uncertainty that characterise many dialogue domains. The thesis also demonstrates how the parameters of probabilistic rules can be efficiently es- timated using both supervised and reinforcement learning techniques. In the case of supervised learning, the rule parameters are learned by imitation on the basis of small amounts of Wizard- of-Oz data. Alternatively, rule parameters can also be optimised via trial and error from repeated interactions with a (real or simulated) user. Both learning strategies rely on Bayesian inference to iteratively estimate the parameter values and provide the best fit for the observed interaction data. Three consecutive experiments conducted in a human–robot interaction domain attest to the practical viability of the proposed framework and its advantages over traditional approaches. In particular, the empirical results of a user evaluation with 37 participants show that a dialogue man- ager structured with probabilistic rules outperforms both purely hand-crafted and purely statistical methods on an extensive range of subjective and objective metrics of dialogue quality. The modelling framework presented in this thesis is implemented in a new software toolkit called OpenDial, which is made freely available to the research community and can be used to develop various types of dialogue systems based on probabilistic rules.},
author = {Lison, Pierre},
file = {:D$\backslash$:/Research/papers/thesis-plison2013.pdf:pdf},
issn = {1501-7710},
number = {October},
title = {{Structured Probabilistic Modelling for Dialogue Management}},
year = {2013}
}
@article{Kaelbling1998,
abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (MDPs) and partially observable MDPs (POMDPs). We then outline a novel algorithm for solving POMDPs off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to POMDPs, and of some possibilities for finding approximate solutions. Consider the problem of a robot navigating in a large office building. The robot can move from hallway intersection to intersection and can make local observations of its world. Its actions are not completely reliable, however. Sometimes, when it intends to move, it stays where it is or goes too far; sometimes, when it intends to turn, it overshoots. It has similar problems with observation. Sometimes a corridor looks like a corner; sometimes a T-junction looks like an L-junction. How can such an error-plagued robot navigate, even given a map of the corridors? PII: S 0 0 0 4 -3 7 0 2 (9 8) 0 0 0 2 3 -X 100 L.P. Kaelbling et al. / Artificial Intelligence 101 (1998) 99–134 In general, the robot will have to remember something about its history of actions and observations and use this information, together with its knowledge of the underlying dynamics of the world (the map and other information), to maintain an estimate of its location. Many engineering applications follow this approach, using methods like the Kalman filter [26] to maintain a running estimate of the robot's spatial uncertainty, expressed as an ellipsoid or normal distribution in Cartesian space. This approach will not do for our robot, though. Its uncertainty may be discrete: it might be almost certain that it is in the north-east corner of either the fourth or the seventh floors, though it admits a chance that it is on the fifth floor, as well. Then, given an uncertain estimate of its location, the robot has to decide what actions to take. In some cases, it might be sufficient to ignore its uncertainty and take actions that would be appropriate for the most likely location. In other cases, it might be better for the robot to take actions for the purpose of gathering information, such as searching for a landmark or reading signs on the wall. In general, it will take actions that fulfill both purposes simultaneously.},
author = {Kaelbling, Leslie Pack and Littman, Michael L and Cassandra, Anthony R},
doi = {10.1016/S0004-3702(98)00023-X},
file = {:D$\backslash$:/Research/papers/aij98-pomdp.pdf:pdf},
isbn = {00043702 (ISSN)},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Partially observable Markov decision processes,Planning,Uncertainty},
pages = {99--134},
pmid = {20552381},
title = {{Planning and acting in partially observable stochastic domains}},
volume = {101},
year = {1998}
}
@article{Henderson2014,
author = {Henderson, Matthew and Thomson, Blaise and Williams, Jason D},
file = {:D$\backslash$:/Research/papers/W13-4065.pdf:pdf},
isbn = {9781479971299},
number = {August},
pages = {2--7},
title = {{the Second Dialog State Tracking Challenge}},
year = {2014}
}
@article{Miller2000,
abstract = {Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard. In this paper we report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.},
author = {Miller, Scott and Fox, Heidi and Ramshaw, Lance and Weischedel, Ralph},
file = {:D$\backslash$:/Research/papers/A00-2030.pdf:pdf},
isbn = {0000000000},
journal = {Syntax And Semantics},
pages = {226--233},
title = {{A novel use of statistical parsing to extract information from text}},
url = {http://portal.acm.org/citation.cfm?id=974335},
year = {2000}
}
@article{Xiong2016a,
abstract = {Conversational speech recognition has served as a flagship speech recognition task since the release of the DARPA Switchboard corpus in the 1990s. In this paper, we measure the human error rate on the widely used NIST 2000 test set, and find that our latest automated system has reached human parity. The error rate of professional transcriptionists is 5.9{\%} for the Switchboard portion of the data, in which newly acquainted pairs of people discuss an assigned topic, and 11.3{\%} for the CallHome portion where friends and family members have open-ended conversations. In both cases, our automated system establishes a new state-of-the-art, and edges past the human benchmark. This marks the first time that human parity has been reported for conversational speech. The key to our system's performance is the systematic use of convolutional and LSTM neural networks, combined with a novel spatial smoothing method and lattice-free MMI acoustic training.},
archivePrefix = {arXiv},
arxivId = {1610.05256},
author = {Xiong, W. and Droppo, J. and Huang, X. and Seide, F. and Seltzer, M. and Stolcke, A. and Yu, D. and Zweig, G.},
eprint = {1610.05256},
file = {:D$\backslash$:/Research/papers/1610.05256v1.pdf:pdf},
title = {{Achieving Human Parity in Conversational Speech Recognition}},
url = {http://arxiv.org/abs/1610.05256},
year = {2016}
}
@article{DanielPovey2014,
abstract = {We describe the neural-network training framework used in the Kaldi speech recognition toolkit, which is geared towards training DNNs with large amounts of training data using multiple GPU-equipped or multi-core machines. In order to be as hardware-agnostic as possible, we needed a way to use multiple machines without generating excessive network traffic. Our method is to average the neural network parameters periodically (typically every minute or two), and redistribute the averaged parameters to the machines for further training. Each machine sees different data. By itself, this method does not work very well. However, we have another method, an approximate and efficient implementation of Natural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow our periodic-averaging method to work well, as well as substantially improving the convergence of SGD on a single machine.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.7455v8},
author = {{Daniel Povey}, Xiaohui Zhang {\&} Sanjeev Khudanpur},
eprint = {arXiv:1410.7455v8},
file = {:D$\backslash$:/Research/papers/1410.7455v4.pdf:pdf},
journal = {Iclr 2015},
number = {March 2013},
pages = {1--2},
title = {{Parallel Training of Dnns With Natural Gradient and Parameter Averaging}},
url = {http://arxiv.org/abs/1410.7455},
year = {2014}
}
@book{Jurafsky2006,
abstract = {Stanford NLP 经典教材},
author = {Jurafsky, Daniel and Martin, James H.},
file = {:D$\backslash$:/Research/papers/语音与语言处理Speech+and+Language+Processing.pdf:pdf},
pages = {1037},
publisher = {Prentice Hall},
title = {{Speech and Language Processing}},
year = {2006}
}
@article{Wen2015a,
abstract = {The natural language generation (NLG) component of a spoken dialogue system (SDS) usually needs a substantial amount of handcrafting or a well-labeled dataset to be trained on. These limitations add sig-nificantly to development costs and make cross-domain, multi-lingual dialogue sys-tems intractable. Moreover, human lan-guages are context-aware. The most nat-ural response should be directly learned from data rather than depending on pre-defined syntaxes or rules. This paper presents a statistical language generator based on a joint recurrent and convolu-tional neural network structure which can be trained on dialogue act-utterance pairs without any semantic alignments or pre-defined grammar trees. Objective metrics suggest that this new model outperforms previous methods under the same experi-mental conditions. Results of an evalua-tion by human judges indicate that it pro-duces not only high quality but linguisti-cally varied utterances which are preferred compared to n-gram and rule-based sys-tems.},
archivePrefix = {arXiv},
arxivId = {1508.01755},
author = {Wen, Tsung-Hsien and Ga{\v{s}}i, Milica and Kim, Dongho and Mrk{\v{s}}i, Nikola and Su, Pei-Hao and Vandyke, David and Young, Steve},
eprint = {1508.01755},
file = {:D$\backslash$:/Research/papers/TR698.pdf:pdf},
isbn = {9781941643754},
journal = {Sigdial Best Paper},
pages = {275--284},
title = {{Stochastic Language Generation in Dialogue using Recurrent Neural Networks with Convolutional Sentence Reranking}},
year = {2015}
}
@article{Mohri2000,
abstract = {This chapter describes a general representation and algorithmic framework for speech recognition based on weighted finite-state transducers. These trans- ducers provide a common and natural representa- tion formajor components of speech recognition sys- tems, including hidden Markov models (HMMs), context-dependency models, pronunciation dictio- naries, statistical grammars, and word or phone lat- tices. General algorithms for building and optimizing transducermodels are presented, including composi- tion for combiningmodels,weighted determinization and minimization for optimizing time and space requirements, and a weight pushing algorithm for re- distributing transition weights optimally for speech recognition. The application of these methods to large-vocabulary recognition tasks is explained in de- tail, and experimental results are given, in particu- lar for the North American Business News (NAB) task, in which these methods were used to combine HMMs, full cross-word triphones, a lexicon of forty thousand words, and a large trigram grammar into a single weighted transducer that is only somewhat larger than the trigram word grammar and that runs NAB in real-time on a very simple decoder. Another example demonstrates that the same methods can be used to optimize lattices for second-pass recognition.},
author = {Mohri, Mehryar and York, New and Riley, Michael},
doi = {10.1007/978-3-540-49127-9{\_}28},
file = {:D$\backslash$:/Research/papers/hbka.pdf:pdf},
isbn = {978-3-540-49125-5},
issn = {08852308},
journal = {Speech Communication},
pages = {1--31},
pmid = {14909180},
title = {{SPEECH RECOGNITION WITH WEIGHTED FINITE-STATE TRANSDUCERS}},
year = {2000}
}
@article{Miller1996,
author = {Miller, Scott and Stallard, David and Bobrow, Robert and Schwartz, Richard},
file = {:D$\backslash$:/Research/papers/P96-1008.pdf:pdf},
journal = {Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL-96)},
pages = {55--61},
title = {{A Fully Statistical Approach to Natural Language Interfaces}},
year = {1996}
}
@article{Singh2002,
abstract = {Designing the dialogue policy of a spoken dialogue system involves many nontrivial choices. This paper presents a reinforcement learning approach for automatically optimizing a dialogue policy, which addresses the technical challenges in applying reinforcement learning to a working dialogue system with human users. We report on the design, construction and empirical evaluation of NJFun, an experimental spoken dialogue system that provides users with access to information about fun things to do in New Jersey. Our results show that by optimizing its performance via reinforcement learning, NJFun measurably improves system performance.},
archivePrefix = {arXiv},
arxivId = {1106.0676},
author = {Singh, Satinder and Man, Diane Lit and Kearns, Michael and Walker, Marilyn},
doi = {10.1613/jair.859},
eprint = {1106.0676},
file = {:D$\backslash$:/Research/papers/RLDSjair.pdf:pdf},
isbn = {10769757},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {105--133},
title = {{Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system}},
volume = {16},
year = {2002}
}
@article{Rayner2003,
author = {Rayner, M},
file = {:D$\backslash$:/Research/papers/E03-1078.pdf:pdf},
journal = {Proceedings of the tenth conference on},
pages = {299--306},
title = {{Transparent combination of rule-based and data-driven approaches in a speech understanding architecture}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/cbdv.200490137/abstract$\backslash$nhttp://dl.acm.org/citation.cfm?id=1067847},
year = {2003}
}
@article{Bordes2016Learning,
abstract = {End-to-end dialog systems, in which all components are learnt simultaneously, have recently obtained encouraging successes. However these were mostly on conversations related to chit-chat with no clear objective and for which evaluation is difficult. This paper proposes a set of tasks to test the capabilities of such systems on goal-oriented dialogs, where goal completion ensures a well-defined measure of performance. Built in the context of restaurant reservation, our tasks require to manipulate sentences and symbols, in order to properly conduct conversations, issue API calls and use the outputs of such calls. We show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations. We confirm those results by comparing our system to a hand-crafted slot-filling baseline on data from the second Dialog State Tracking Challenge (Henderson et al., 2014a).},
archivePrefix = {arXiv},
arxivId = {1605.07683},
author = {Bordes, Antoine and Weston, Jason},
eprint = {1605.07683},
file = {:C$\backslash$:/Users/lenovo/Desktop/1605.07683.pdf:pdf},
title = {{Learning End-to-End Goal-Oriented Dialog}},
url = {http://arxiv.org/abs/1605.07683},
year = {2016}
}
@article{Young2013Pomdp,
abstract = {—Statistical dialogue systems are motivated by the need for a data-driven framework that reduces the cost of laboriously hand-crafting complex dialogue managers and that provides robustness against the errors created by speech recog-nisers operating in noisy environments. By including an explicit Bayesian model of uncertainty and by optimising the policy via a reward-driven process, partially observable Markov decision processes (POMDPs) provide such a framework. However, ex-act model representation and optimisation is computationally intractable. Hence, the practical application of POMDP-based systems requires efficient algorithms and carefully constructed approximations. This review article provides an overview of the current state of the art in the development of POMDP-based spoken dialogue systems.},
author = {Young, Steve and Ga{\v{s}}i, Milica and Thomson, Blaise and Williams, Jason D},
doi = {10.1016/j.csl.2009.04.001},
file = {:D$\backslash$:/Research/papers/ygtw13.pdf:pdf},
isbn = {0885-2308},
issn = {08852308},
journal = {Proc IEEE},
keywords = {Index Terms—Spoken dialogue systems,POMDP,belief monitoring,policy optimisation,reinforce-ment learning},
number = {5},
pages = {1160--1179},
title = {{POMDP-based Statistical Spoken Dialogue Systems: a Review}},
volume = {101},
year = {2013}
}
@article{Williams2013,
abstract = {In a spoken dialog system, dialog state tracking deduces information about the user's goal as the dialog progresses, syn-thesizing evidence such as dialog acts over multiple turns with external data sources. Recent approaches have been shown to overcome ASR and SLU errors in some applications. However, there are currently no common testbeds or evaluation mea-sures for this task, hampering progress. The dialog state tracking challenge seeks to address this by providing a heteroge-neous corpus of 15K human-computer di-alogs in a standard format, along with a suite of 11 evaluation metrics. The chal-lenge received a total of 27 entries from 9 research groups. The results show that the suite of performance metrics cluster into 4 natural groups. Moreover, the dialog sys-tems that benefit most from dialog state tracking are those with less discriminative speech recognition confidence scores. Fi-nally, generalization is a key problem: in 2 of the 4 test sets, fewer than half of the entries out-performed simple baselines.},
author = {Williams, Jason and Raux, Antoine and Ramachandran, Deepak and Black, Alan},
doi = {10.5087/dad.2016.301},
file = {:D$\backslash$:/Research/papers/williams2016dstc{\_}overview-1.pdf:pdf},
journal = {Sigdial},
keywords = {conversational sys-,dialog modeling,dialog state tracking,spoken dialog systems,spoken language understanding,tems},
number = {August},
pages = {404--413},
title = {{The Dialog State Tracking Challenge: A Review}},
volume = {7},
year = {2013}
}
